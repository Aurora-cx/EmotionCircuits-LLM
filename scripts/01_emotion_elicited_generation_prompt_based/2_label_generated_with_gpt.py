# scripts/01_emotion_elicited_generation_prompt_based/2_label_generated_with_gpt.py
# -*- coding: utf-8 -*-
"""
æƒ…ç»ªæ–‡æœ¬æ‰“æ ‡è„šæœ¬
Emotion Text Labeling Script

æ‰¹é‡æ‰“æ ‡è„šæœ¬ï¼šè¯»å–è„šæœ¬1ç”Ÿæˆçš„æ–‡æœ¬ï¼Œä½¿ç”¨ GPT-4o-mini æ‰“æ ‡ï¼ŒæŒ‰ accepted/rejected åˆ†ç±»ä¿å­˜
Batch labeling script: Reads texts generated by script 1, uses GPT-4o-mini to label, saves by accepted/rejected

- è¾“å…¥ Input: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/generated/{dataset_name}_generated.jsonl
- è¾“å‡º Output: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/labeled/{dataset_name}/{accepted|rejected}.jsonl
"""

import os, json, time, argparse
from pathlib import Path
from typing import Dict, Any

from openai import OpenAI

# ====== OpenAI (GPT-4o-mini æ‰“æ ‡) ======
client = OpenAI(
    api_key="Your OpenAI API Key",
)

EMOTIONS = ["anger","sadness","happiness","fear","surprise","disgust"]

SYSTEM_LBL = f'''
You are a careful rater.
Given a target emotion and a text,
decide if the text's STYLE matches the target emotion among:
{EMOTIONS}
Focus on tone/attitude, not content valence.
'''.strip()

USER_TMPL_LBL = '''
Target emotion: {emotion}
Text:
{text}
Decide if the text's STYLE matches the target emotion.
Return a compact JSON with keys exactly:
{{
"match": <0 or 1>,
"reason": <short string>
}}
'''.strip()


def extract_json_from_response(response: str) -> str:
    """
    ä»GPTå“åº”ä¸­æå–JSONå†…å®¹ï¼Œå¤„ç†markdownæ ¼å¼
    Extract JSON content from GPT response, handling markdown format
    """
    response = response.strip()
    
    # å¦‚æœåŒ…å«markdownä»£ç å—ï¼Œæå–å…¶ä¸­çš„JSON
    # If contains markdown code block, extract JSON from it
    if "```json" in response:
        start = response.find("```json") + 7
        end = response.find("```", start)
        if end != -1:
            return response[start:end].strip()
    elif "```" in response:
        # å¤„ç†æ²¡æœ‰jsonæ ‡è¯†çš„ä»£ç å—
        # Handle code blocks without json identifier
        start = response.find("```") + 3
        end = response.find("```", start)
        if end != -1:
            return response[start:end].strip()
    
    # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œç›´æ¥è¿”å›åŸå†…å®¹
    # If no code block, return original content
    return response


def ask_llm_label(client, model: str, emotion: str, text: str,
                  max_retries=4, backoff=1.8) -> Dict[str, Any]:
    """
    è°ƒç”¨ GPT-4o-mini æ‰“æ ‡ï¼›æ—  KEY æˆ–é”™è¯¯åˆ™è¿”å›æœªæ ‡æ³¨
    Call GPT-4o-mini for labeling; return unlabeled if no KEY or error
    """
    for i in range(max_retries):
        try:
            user_content = USER_TMPL_LBL.format(emotion=emotion, text=text)
            try:
                resp = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": SYSTEM_LBL},
                        {"role": "user",   "content": user_content}
                    ],
                )
            except Exception as api_error:
                if i == max_retries - 1:
                    return {"match": 0, "reason": f"api-error:{type(api_error).__name__}"}
                time.sleep(backoff ** i)
                continue
            
            # å®‰å…¨åœ°è·å–å“åº”å†…å®¹
            # Safely get response content
            try:
                choice = resp.choices[0]
                message = choice.message
                content = message.content
                out = (content or "").strip()
            except (KeyError, IndexError, AttributeError) as ke:
                return {"match": 0, "reason": f"response-structure-error: {type(ke).__name__}"}
            
            js = extract_json_from_response(out)
            try:
                j = json.loads(js)
                if "match" not in j: j = {"match": 0, "reason": "invalid-json"}
                if "reason" not in j: j["reason"] = "no-reason-provided"
                return j
            except json.JSONDecodeError as je:
                return {"match": 0, "reason": f"json-decode-error: {str(je)}"}
        except Exception as e:
            if i == max_retries - 1:
                return {"match": 0, "reason": f"error:{type(e).__name__}"}
            time.sleep(backoff ** i)
    
    return {"match": 0, "reason": "max-retries-exceeded"}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True,
                       help="è¾“å…¥æ•°æ®è·¯å¾„ Input data pathï¼Œå¦‚ outputs/llama32_3b/01_emotion_elicited_generation_prompt_based/generated/sev_generated.jsonl")
    parser.add_argument("--lbl_model", type=str, default="gpt-4o-mini",
                       help="æ‰“æ ‡æ¨¡å‹ Label model")
    parser.add_argument("--skip_if_exists", action="store_true", default=True,
                       help="è·³è¿‡å·²æ‰“æ ‡çš„é¡¹ç›® Skip already labeled items")
    parser.add_argument("--no_skip", action="store_true",
                       help="é‡æ–°æ‰“æ ‡æ‰€æœ‰é¡¹ç›® Relabel all items")
    args = parser.parse_args()

    # è§£æè¾“å…¥è·¯å¾„ï¼Œè‡ªåŠ¨æ¨æ–­è¾“å‡ºè·¯å¾„
    # Parse input path and infer output path
    # è¾“å…¥æ ¼å¼: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/generated/{dataset_name}_generated.jsonl
    # è¾“å‡ºæ ¼å¼: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/labeled/{dataset_name}/{accepted|rejected}.jsonl
    input_path = Path(args.input_path)
    
    if not input_path.exists():
        print(f"[ERROR] Input file not found: {input_path}")
        return
    
    # ä»è¾“å…¥è·¯å¾„æå– model_name å’Œ dataset_name
    # Extract model_name and dataset_name from input path
    parts = input_path.parts
    if "outputs" in parts and "01_emotion_elicited_generation_prompt_based" in parts and "generated" in parts:
        outputs_idx = parts.index("outputs")
        model_name = parts[outputs_idx + 1]
        # ä»æ–‡ä»¶åæå–dataset_name (ä¾‹å¦‚: sev_generated.jsonl -> sev)
        filename = input_path.stem  # å»æ‰.jsonl
        if filename.endswith("_generated"):
            dataset_name = filename[:-10]  # å»æ‰_generatedåç¼€
        else:
            dataset_name = filename
    else:
        print(f"[ERROR] Input path format incorrect. Expected: outputs/{{model_name}}/01_emotion_elicited_generation_prompt_based/generated/{{dataset_name}}_generated.jsonl")
        return
    
    # æ„å»ºè¾“å‡ºè·¯å¾„
    # Build output path
    out_dir = Path("outputs") / model_name / "01_emotion_elicited_generation_prompt_based" / "labeled" / dataset_name
    out_dir.mkdir(parents=True, exist_ok=True)

    acc_path = out_dir / "accepted.jsonl"
    rej_path = out_dir / "rejected.jsonl"

    # åŠ è½½å·²æ‰“æ ‡çš„ keysï¼ˆç”¨äºæ–­ç‚¹ç»­è·‘ï¼‰
    # Load existing keys (for resuming from checkpoint)
    existing_keys = set()
    if args.skip_if_exists and not args.no_skip:
        for path in [acc_path, rej_path]:
            if path.exists():
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        try:
                            obj = json.loads(line.strip())
                            if "key" in obj:
                                existing_keys.add(obj["key"])
                        except:
                            continue

    total = 0
    skipped = 0
    accepted = 0
    rejected = 0
    start = time.time()

    # ç»Ÿè®¡å­—å…¸ï¼šæŒ‰æƒ…ç»ªå’Œææ€§åˆ†ç±»
    # Statistics dictionaries: by emotion and valence
    stats_by_emotion = {}  # {emotion: {"total": N, "accepted": M}}
    stats_by_valence = {}  # {valence: {"total": N, "accepted": M}}

    with open(input_path, "r", encoding="utf-8") as fin:
        for line in fin:
            line = line.strip()
            if not line: continue
            
            try:
                item = json.loads(line)
            except json.JSONDecodeError:
                print(f"[WARN] Failed to parse line: {line[:100]}...")
                continue

            key = item.get("key", "unknown")
            
            # æ–­ç‚¹ç»­è·‘æ£€æŸ¥
            # Checkpoint resuming check
            if key in existing_keys:
                skipped += 1
                if skipped % 50 == 0:
                    print(f"[SKIP] {skipped} items skipped so far... (last: {key})")
                continue

            emotion = item.get("emotion", "")
            valence = item.get("valence", "")
            gen_text = item.get("gen_text", "")

            # GPT æ‰“æ ‡
            # GPT labeling
            label = {"match": 0, "reason": "empty-text"}
            if isinstance(gen_text, str) and gen_text:
                label = ask_llm_label(client, args.lbl_model, emotion, gen_text)

            # æ·»åŠ æ‰“æ ‡ç»“æœå’Œæ‰“æ ‡æ—¶é—´
            # Add labeling result and timestamp
            item["judge"] = label
            item["label_time"] = int(time.time())

            # æ ¹æ®æ‰“æ ‡ç»“æœä¿å­˜
            # Save based on labeling result
            match_score = int(label.get("match", 0))
            if match_score == 1:
                output_path = acc_path
                accepted += 1
                category = "accepted"
            else:
                output_path = rej_path
                rejected += 1
                category = "rejected"

            with open(output_path, "a", encoding="utf-8") as fout:
                fout.write(json.dumps(item, ensure_ascii=False) + "\n")

            # æ›´æ–°ç»Ÿè®¡
            # Update statistics
            if emotion:
                if emotion not in stats_by_emotion:
                    stats_by_emotion[emotion] = {"total": 0, "accepted": 0}
                stats_by_emotion[emotion]["total"] += 1
                stats_by_emotion[emotion]["accepted"] += match_score

            if valence:
                if valence not in stats_by_valence:
                    stats_by_valence[valence] = {"total": 0, "accepted": 0}
                stats_by_valence[valence]["total"] += 1
                stats_by_valence[valence]["accepted"] += match_score

            total += 1
            if total % 10 == 0:
                el = time.time() - start
                rate = total / el if el > 0 else 0
                print(f"[progress] labeled={total} (acc={accepted}, rej={rejected}) | last={key} [{category}] | {el:.1f}s | {rate:.2f} items/s")

    elapsed = time.time() - start
    print(f"\n[OK] Done. Labeled {total} items, skipped {skipped} items.")
    print(f"     Accepted: {accepted} | Rejected: {rejected}")
    print(f"     Time: {elapsed:.1f}s | Rate: {total/elapsed:.2f} items/s")
    print(f"     Output:")
    print(f"       - {acc_path}")
    print(f"       - {rej_path}")

    # ===== è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ =====
    # ===== Output statistics =====
    print("\n" + "="*60)
    print("ğŸ“Š ACCURACY STATISTICS")
    print("="*60)

    # æ€»ä½“æ­£ç¡®ç‡
    # Overall accuracy
    overall_acc = (accepted / total * 100) if total > 0 else 0
    print(f"\nğŸ¯ Overall Accuracy: {accepted}/{total} = {overall_acc:.2f}%")

    # æŒ‰æƒ…ç»ªç»Ÿè®¡
    # Statistics by emotion
    print(f"\nğŸ“ˆ Accuracy by Emotion:")
    print("-" * 60)
    print(f"{'Emotion':<15} {'Accepted':<12} {'Total':<10} {'Accuracy':<12}")
    print("-" * 60)
    
    emotions_sorted = sorted(stats_by_emotion.items())
    for emotion, stats in emotions_sorted:
        acc_count = stats["accepted"]
        tot_count = stats["total"]
        acc_rate = (acc_count / tot_count * 100) if tot_count > 0 else 0
        print(f"{emotion:<15} {acc_count:<12} {tot_count:<10} {acc_rate:>6.2f}%")
    
    # æŒ‰ææ€§ç»Ÿè®¡
    # Statistics by valence
    print(f"\nğŸ“‰ Accuracy by Valence:")
    print("-" * 60)
    print(f"{'Valence':<15} {'Accepted':<12} {'Total':<10} {'Accuracy':<12}")
    print("-" * 60)
    
    valences_sorted = sorted(stats_by_valence.items())
    for valence, stats in valences_sorted:
        acc_count = stats["accepted"]
        tot_count = stats["total"]
        acc_rate = (acc_count / tot_count * 100) if tot_count > 0 else 0
        print(f"{valence:<15} {acc_count:<12} {tot_count:<10} {acc_rate:>6.2f}%")
    
    print("="*60)


if __name__ == "__main__":
    main()

