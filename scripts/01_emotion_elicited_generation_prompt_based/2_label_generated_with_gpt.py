# scripts/01_emotion_elicited_generation_prompt_based/2_label_generated_with_gpt.py
# -*- coding: utf-8 -*-
"""
情绪文本打标脚本
Emotion Text Labeling Script

批量打标脚本：读取脚本1生成的文本，使用 GPT-4o-mini 打标，按 accepted/rejected 分类保存
Batch labeling script: Reads texts generated by script 1, uses GPT-4o-mini to label, saves by accepted/rejected

- 输入 Input: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/generated/{dataset_name}_generated.jsonl
- 输出 Output: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/labeled/{dataset_name}/{accepted|rejected}.jsonl
"""

import os, json, time, argparse
from pathlib import Path
from typing import Dict, Any

from openai import OpenAI

# ====== OpenAI (GPT-4o-mini 打标) ======
client = OpenAI(
    api_key="Your OpenAI API Key",
)

EMOTIONS = ["anger","sadness","happiness","fear","surprise","disgust"]

SYSTEM_LBL = f'''
You are a careful rater.
Given a target emotion and a text,
decide if the text's STYLE matches the target emotion among:
{EMOTIONS}
Focus on tone/attitude, not content valence.
'''.strip()

USER_TMPL_LBL = '''
Target emotion: {emotion}
Text:
{text}
Decide if the text's STYLE matches the target emotion.
Return a compact JSON with keys exactly:
{{
"match": <0 or 1>,
"reason": <short string>
}}
'''.strip()


def extract_json_from_response(response: str) -> str:
    """
    从GPT响应中提取JSON内容，处理markdown格式
    Extract JSON content from GPT response, handling markdown format
    """
    response = response.strip()
    
    # 如果包含markdown代码块，提取其中的JSON
    # If contains markdown code block, extract JSON from it
    if "```json" in response:
        start = response.find("```json") + 7
        end = response.find("```", start)
        if end != -1:
            return response[start:end].strip()
    elif "```" in response:
        # 处理没有json标识的代码块
        # Handle code blocks without json identifier
        start = response.find("```") + 3
        end = response.find("```", start)
        if end != -1:
            return response[start:end].strip()
    
    # 如果没有代码块，直接返回原内容
    # If no code block, return original content
    return response


def ask_llm_label(client, model: str, emotion: str, text: str,
                  max_retries=4, backoff=1.8) -> Dict[str, Any]:
    """
    调用 GPT-4o-mini 打标；无 KEY 或错误则返回未标注
    Call GPT-4o-mini for labeling; return unlabeled if no KEY or error
    """
    for i in range(max_retries):
        try:
            user_content = USER_TMPL_LBL.format(emotion=emotion, text=text)
            try:
                resp = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": SYSTEM_LBL},
                        {"role": "user",   "content": user_content}
                    ],
                )
            except Exception as api_error:
                if i == max_retries - 1:
                    return {"match": 0, "reason": f"api-error:{type(api_error).__name__}"}
                time.sleep(backoff ** i)
                continue
            
            # 安全地获取响应内容
            # Safely get response content
            try:
                choice = resp.choices[0]
                message = choice.message
                content = message.content
                out = (content or "").strip()
            except (KeyError, IndexError, AttributeError) as ke:
                return {"match": 0, "reason": f"response-structure-error: {type(ke).__name__}"}
            
            js = extract_json_from_response(out)
            try:
                j = json.loads(js)
                if "match" not in j: j = {"match": 0, "reason": "invalid-json"}
                if "reason" not in j: j["reason"] = "no-reason-provided"
                return j
            except json.JSONDecodeError as je:
                return {"match": 0, "reason": f"json-decode-error: {str(je)}"}
        except Exception as e:
            if i == max_retries - 1:
                return {"match": 0, "reason": f"error:{type(e).__name__}"}
            time.sleep(backoff ** i)
    
    return {"match": 0, "reason": "max-retries-exceeded"}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True,
                       help="输入数据路径 Input data path，如 outputs/llama32_3b/01_emotion_elicited_generation_prompt_based/generated/sev_generated.jsonl")
    parser.add_argument("--lbl_model", type=str, default="gpt-4o-mini",
                       help="打标模型 Label model")
    parser.add_argument("--skip_if_exists", action="store_true", default=True,
                       help="跳过已打标的项目 Skip already labeled items")
    parser.add_argument("--no_skip", action="store_true",
                       help="重新打标所有项目 Relabel all items")
    args = parser.parse_args()

    # 解析输入路径，自动推断输出路径
    # Parse input path and infer output path
    # 输入格式: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/generated/{dataset_name}_generated.jsonl
    # 输出格式: outputs/{model_name}/01_emotion_elicited_generation_prompt_based/labeled/{dataset_name}/{accepted|rejected}.jsonl
    input_path = Path(args.input_path)
    
    if not input_path.exists():
        print(f"[ERROR] Input file not found: {input_path}")
        return
    
    # 从输入路径提取 model_name 和 dataset_name
    # Extract model_name and dataset_name from input path
    parts = input_path.parts
    if "outputs" in parts and "01_emotion_elicited_generation_prompt_based" in parts and "generated" in parts:
        outputs_idx = parts.index("outputs")
        model_name = parts[outputs_idx + 1]
        # 从文件名提取dataset_name (例如: sev_generated.jsonl -> sev)
        filename = input_path.stem  # 去掉.jsonl
        if filename.endswith("_generated"):
            dataset_name = filename[:-10]  # 去掉_generated后缀
        else:
            dataset_name = filename
    else:
        print(f"[ERROR] Input path format incorrect. Expected: outputs/{{model_name}}/01_emotion_elicited_generation_prompt_based/generated/{{dataset_name}}_generated.jsonl")
        return
    
    # 构建输出路径
    # Build output path
    out_dir = Path("outputs") / model_name / "01_emotion_elicited_generation_prompt_based" / "labeled" / dataset_name
    out_dir.mkdir(parents=True, exist_ok=True)

    acc_path = out_dir / "accepted.jsonl"
    rej_path = out_dir / "rejected.jsonl"

    # 加载已打标的 keys（用于断点续跑）
    # Load existing keys (for resuming from checkpoint)
    existing_keys = set()
    if args.skip_if_exists and not args.no_skip:
        for path in [acc_path, rej_path]:
            if path.exists():
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        try:
                            obj = json.loads(line.strip())
                            if "key" in obj:
                                existing_keys.add(obj["key"])
                        except:
                            continue

    total = 0
    skipped = 0
    accepted = 0
    rejected = 0
    start = time.time()

    # 统计字典：按情绪和极性分类
    # Statistics dictionaries: by emotion and valence
    stats_by_emotion = {}  # {emotion: {"total": N, "accepted": M}}
    stats_by_valence = {}  # {valence: {"total": N, "accepted": M}}

    with open(input_path, "r", encoding="utf-8") as fin:
        for line in fin:
            line = line.strip()
            if not line: continue
            
            try:
                item = json.loads(line)
            except json.JSONDecodeError:
                print(f"[WARN] Failed to parse line: {line[:100]}...")
                continue

            key = item.get("key", "unknown")
            
            # 断点续跑检查
            # Checkpoint resuming check
            if key in existing_keys:
                skipped += 1
                if skipped % 50 == 0:
                    print(f"[SKIP] {skipped} items skipped so far... (last: {key})")
                continue

            emotion = item.get("emotion", "")
            valence = item.get("valence", "")
            gen_text = item.get("gen_text", "")

            # GPT 打标
            # GPT labeling
            label = {"match": 0, "reason": "empty-text"}
            if isinstance(gen_text, str) and gen_text:
                label = ask_llm_label(client, args.lbl_model, emotion, gen_text)

            # 添加打标结果和打标时间
            # Add labeling result and timestamp
            item["judge"] = label
            item["label_time"] = int(time.time())

            # 根据打标结果保存
            # Save based on labeling result
            match_score = int(label.get("match", 0))
            if match_score == 1:
                output_path = acc_path
                accepted += 1
                category = "accepted"
            else:
                output_path = rej_path
                rejected += 1
                category = "rejected"

            with open(output_path, "a", encoding="utf-8") as fout:
                fout.write(json.dumps(item, ensure_ascii=False) + "\n")

            # 更新统计
            # Update statistics
            if emotion:
                if emotion not in stats_by_emotion:
                    stats_by_emotion[emotion] = {"total": 0, "accepted": 0}
                stats_by_emotion[emotion]["total"] += 1
                stats_by_emotion[emotion]["accepted"] += match_score

            if valence:
                if valence not in stats_by_valence:
                    stats_by_valence[valence] = {"total": 0, "accepted": 0}
                stats_by_valence[valence]["total"] += 1
                stats_by_valence[valence]["accepted"] += match_score

            total += 1
            if total % 10 == 0:
                el = time.time() - start
                rate = total / el if el > 0 else 0
                print(f"[progress] labeled={total} (acc={accepted}, rej={rejected}) | last={key} [{category}] | {el:.1f}s | {rate:.2f} items/s")

    elapsed = time.time() - start
    print(f"\n[OK] Done. Labeled {total} items, skipped {skipped} items.")
    print(f"     Accepted: {accepted} | Rejected: {rejected}")
    print(f"     Time: {elapsed:.1f}s | Rate: {total/elapsed:.2f} items/s")
    print(f"     Output:")
    print(f"       - {acc_path}")
    print(f"       - {rej_path}")

    # ===== 输出统计信息 =====
    # ===== Output statistics =====
    print("\n" + "="*60)
    print("📊 ACCURACY STATISTICS")
    print("="*60)

    # 总体正确率
    # Overall accuracy
    overall_acc = (accepted / total * 100) if total > 0 else 0
    print(f"\n🎯 Overall Accuracy: {accepted}/{total} = {overall_acc:.2f}%")

    # 按情绪统计
    # Statistics by emotion
    print(f"\n📈 Accuracy by Emotion:")
    print("-" * 60)
    print(f"{'Emotion':<15} {'Accepted':<12} {'Total':<10} {'Accuracy':<12}")
    print("-" * 60)
    
    emotions_sorted = sorted(stats_by_emotion.items())
    for emotion, stats in emotions_sorted:
        acc_count = stats["accepted"]
        tot_count = stats["total"]
        acc_rate = (acc_count / tot_count * 100) if tot_count > 0 else 0
        print(f"{emotion:<15} {acc_count:<12} {tot_count:<10} {acc_rate:>6.2f}%")
    
    # 按极性统计
    # Statistics by valence
    print(f"\n📉 Accuracy by Valence:")
    print("-" * 60)
    print(f"{'Valence':<15} {'Accepted':<12} {'Total':<10} {'Accuracy':<12}")
    print("-" * 60)
    
    valences_sorted = sorted(stats_by_valence.items())
    for valence, stats in valences_sorted:
        acc_count = stats["accepted"]
        tot_count = stats["total"]
        acc_rate = (acc_count / tot_count * 100) if tot_count > 0 else 0
        print(f"{valence:<15} {acc_count:<12} {tot_count:<10} {acc_rate:>6.2f}%")
    
    print("="*60)


if __name__ == "__main__":
    main()

