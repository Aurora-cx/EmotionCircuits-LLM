# scripts/03_emotion_elicited_generation_steer_based/3_generate_accuracy_stats.py
# -*- coding: utf-8 -*-
"""
æƒ…ç»ªå¼•å¯¼ç”Ÿæˆå‡†ç¡®ç‡ç»Ÿè®¡è„šæœ¬
Emotion-Steered Generation Accuracy Statistics Script

è¯»å–GPTæ‰“æ ‡ç»“æœï¼ŒæŒ‰æƒ…ç»ªå’Œææ€§åˆ†ç±»ç»Ÿè®¡å‡†ç¡®ç‡
Reads GPT labeling results, calculates accuracy by emotion and valence

è¾“å…¥ Input: outputs/{model_name}/03_emotion_steered_generation/{dataset_name}/labeled_results.jsonl
è¾“å‡º Output: outputs/{model_name}/03_emotion_steered_generation/{dataset_name}/accuracy_stats.json
"""

import json, argparse
from pathlib import Path
from collections import defaultdict

def generate_accuracy_stats(output_dir: Path, dataset_name: str):
    """
    ä¸ºæŒ‡å®šæ•°æ®é›†ç”Ÿæˆå‡†ç¡®ç‡ç»Ÿè®¡
    Generate accuracy statistics for specified dataset
    """
    
    dataset_dir = output_dir / dataset_name
    labeled_path = dataset_dir / "labeled_results.jsonl"
    stats_path = dataset_dir / "accuracy_stats.json"
    
    if not labeled_path.exists():
        print(f"[ERROR] Labeled results file not found: {labeled_path}")
        return None
    
    # ç»Ÿè®¡å­—å…¸
    # Statistics dictionaries
    stats_by_emotion = defaultdict(lambda: {"total": 0, "matches": 0})
    stats_by_valence = defaultdict(lambda: {"total": 0, "matches": 0})
    
    total_pairs = 0
    total_matches = 0
    
    # è¯»å–labeled_results
    # Read labeled_results
    print(f"Reading {labeled_path}...")
    with open(labeled_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                
                # valence_labels åŒ…å« positive, neutral, negative ä¸‰ä¸ªé”®
                # valence_labels contains positive, neutral, negative keys
                valence_labels = item.get("valence_labels", {})
                
                # éå†æ¯ä¸ªææ€§
                # Iterate through each valence
                for event_valence, emotion_labels in valence_labels.items():
                    # ç»Ÿè®¡æ¯ä¸ªæƒ…ç»ªçš„åŒ¹é…ç»“æœ
                    # Count matching results for each emotion
                    for emotion, label in emotion_labels.items():
                        match = label.get("match", 0)
                        
                        stats_by_emotion[emotion]["total"] += 1
                        stats_by_valence[event_valence]["total"] += 1
                        total_pairs += 1
                        
                        if match == 1:
                            stats_by_emotion[emotion]["matches"] += 1
                            stats_by_valence[event_valence]["matches"] += 1
                            total_matches += 1
                        
            except json.JSONDecodeError:
                continue
    
    # è®¡ç®—å‡†ç¡®ç‡
    # Calculate accuracy
    for emotion in stats_by_emotion:
        total = stats_by_emotion[emotion]["total"]
        matches = stats_by_emotion[emotion]["matches"]
        stats_by_emotion[emotion]["accuracy"] = round(matches / total * 100, 2) if total > 0 else 0.0
    
    for valence in stats_by_valence:
        total = stats_by_valence[valence]["total"]
        matches = stats_by_valence[valence]["matches"]
        stats_by_valence[valence]["accuracy"] = round(matches / total * 100, 2) if total > 0 else 0.0
    
    # æ€»ä½“ç»Ÿè®¡
    # Overall statistics
    overall_accuracy = round(total_matches / total_pairs * 100, 2) if total_pairs > 0 else 0.0
    
    # æ„å»ºç»Ÿè®¡ç»“æœ
    # Build statistics result
    stats = {
        "dataset": dataset_name,
        "overall": {
            "total_pairs": total_pairs,
            "matches": total_matches,
            "accuracy": overall_accuracy
        },
        "by_emotion": dict(stats_by_emotion),
        "by_valence": dict(stats_by_valence)
    }
    
    # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶
    # Save statistics file
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    return stats, stats_path


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, default="llama32_3b",
                       help="æ¨¡å‹æ–‡ä»¶å¤¹åç§° Model folder name")
    parser.add_argument("--dataset", type=str, default="test_set",
                       help="æ•°æ®é›†åç§° Dataset name (å¦‚ test_set, user_test)ï¼Œä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰æ•°æ®é›†")
    parser.add_argument("--all", action="store_true",
                       help="å¤„ç†æ‰€æœ‰æ•°æ®é›† Process all datasets")
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å‡ºç›®å½•è·¯å¾„
    # Determine output directory path
    output_dir = Path("outputs") / args.model_name / "03_emotion_steered_generation"
    
    if not output_dir.exists():
        print(f"[ERROR] Output directory not found: {output_dir}")
        return
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    # Determine datasets to process
    if args.all:
        # è‡ªåŠ¨å‘ç°æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶å¤¹
        # Auto-discover all dataset folders
        datasets = [d.name for d in output_dir.iterdir() if d.is_dir() and (d / "labeled_results.jsonl").exists()]
    elif args.dataset:
        datasets = [args.dataset]
    else:
        # é»˜è®¤å¤„ç†æ‰€æœ‰åŒ…å«labeled_results.jsonlçš„æ•°æ®é›†
        # Default: process all datasets with labeled_results.jsonl
        datasets = [d.name for d in output_dir.iterdir() if d.is_dir() and (d / "labeled_results.jsonl").exists()]
    
    if not datasets:
        print(f"[ERROR] No datasets with labeled_results.jsonl found in {output_dir}")
        return
    
    print("=" * 70)
    print("ç”Ÿæˆæƒ…ç»ªå¼•å¯¼ç”Ÿæˆå‡†ç¡®ç‡ç»Ÿè®¡ | Generating Emotion Steering Accuracy Stats")
    print("=" * 70)
    
    for dataset_name in datasets:
        result = generate_accuracy_stats(output_dir, dataset_name)
        
        if result:
            stats, stats_path = result
            
            print(f"\nğŸ“Š {dataset_name.upper()} æ•°æ®é›†ç»Ÿè®¡ Dataset Statistics:")
            print(f"   æ–‡ä»¶ File: {stats_path}")
            print(f"   æ€»é…å¯¹æ•° Total Pairs: {stats['overall']['total_pairs']}")
            print(f"   åŒ¹é…æ•° Matches: {stats['overall']['matches']}")
            print(f"   æ€»ä½“å‡†ç¡®ç‡ Overall Accuracy: {stats['overall']['accuracy']}%")
            
            print(f"\n   æŒ‰æƒ…ç»ªåˆ†ç±» By Emotion:")
            for emotion in sorted(stats['by_emotion'].keys()):
                e_stats = stats['by_emotion'][emotion]
                print(f"      {emotion:12s}: {e_stats['matches']:4d}/{e_stats['total']:4d} = {e_stats['accuracy']:6.2f}%")
            
            if stats['by_valence']:
                print(f"\n   æŒ‰ææ€§åˆ†ç±» By Valence:")
                for valence in sorted(stats['by_valence'].keys()):
                    v_stats = stats['by_valence'][valence]
                    print(f"      {valence:12s}: {v_stats['matches']:4d}/{v_stats['total']:4d} = {v_stats['accuracy']:6.2f}%")
    
    print("\n" + "=" * 70)
    print("âœ… ç»Ÿè®¡æ–‡ä»¶ç”Ÿæˆå®Œæˆ! | Statistics generation completed!")
    print("=" * 70)


if __name__ == "__main__":
    main()

